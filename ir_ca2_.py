# -*- coding: utf-8 -*-
"""IR - CA2 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wg9BDhNAvh3Yp2ABwXoSbfmnayyL5raY

##Duplicate detection -> Minhash
"""

# 1. MINHASH using random permutation

import numpy as np
import random

def get_shingles(d, k):
  words = d.split()
  shingles = set()
  for i in range(len(words)-k+1):
    sh = words[i:i+k]
    shingle = " ".join(sh)
    shingles.add(shingle)

  return shingles


def jaccard_similarity(signature, doc1, doc2):
  equal_rows = signature[:,doc1]==signature[:,doc2]
  return np.mean(equal_rows)

docs = [
    "the cat sat on the mat",
    "the dog sat on the mat",
    "the cat chased the dog"
]

k=2 #shingle size

shingles_set = set()
doc_shingles = []

for doc in docs:
  sh = get_shingles(doc, k)
  doc_shingles.append(sh)
  shingles_set |= sh

#print(shingles_set)

shingles_list = list(shingles_set)

print("Doc - shingle")
print(doc_shingles)


print("Unique shingles")
print(shingles_list)

#Shingle document matrix
matrix = []

for shingle in shingles_list:
  row = []
  for i in range(len(docs)):
    if shingle in doc_shingles[i]:
      row.append(1)
    else:
      row.append(0)

  matrix.append(row)

sd_matrix = np.array(matrix)
print("\n Shingle - doc matrix:")
print(sd_matrix)


#Signature matrix

num_shingles, num_docs = sd_matrix.shape
num_hash = 5
signature_matrix = np.full((num_hash, num_docs), np.inf)

rows = list(range(num_shingles))

permutations = [random.sample(rows, len(rows)) for _ in range(num_hash)]

for i, perm in enumerate(permutations):
  for col in range(len(docs)):
    for row in perm:
      if sd_matrix[row][col] == 1:
        signature_matrix[i][col] = row
        break

print("\nSignature matrix")
print(signature_matrix)

#Jaccard similarity between the documents

print(f"Jaccard similarity : doc0 vs doc1: {jaccard_similarity(signature_matrix, 0, 1)} ")
print(f"Jaccard similarity : doc0 vs doc2: {jaccard_similarity(signature_matrix, 0, 2)} ")
print(f"Jaccard similarity : doc1 vs doc2: {jaccard_similarity(signature_matrix, 1, 2)} ")

# 2. MINHASH using hash functions

def get_shingles(doc, k):
  words = doc.split()
  shingles = set()
  for i in range(len(words)-k+1):
    sh = words[i:i+k]
    shingle = " ".join(sh)
    shingles.add(shingle)

  return shingles


def jaccard_similarity(signature, doc1, doc2):
  equal_rows = signature[:, doc1]==signature[:, doc2]
  return np.mean(equal_rows)

docs = [
    "the cat sat on the mat",
    "the dog sat on the mat",
    "the cat chased the dog"
]

k=2

doc_shingles = []
shingle_set = set()

for doc in docs:
  sh = get_shingles(doc, k)
  doc_shingles.append(sh)
  shingle_set |= sh


shingles_list = list(shingles_set)

print("Doc - shingle")
print(doc_shingles)


print("Unique shingles")
print(shingles_list)

matrix = []

for shingle in shingles_list:
  row = []
  for i in range(len(docs)):
    if shingle in doc_shingles[i]:
      row.append(1)
    else:
      row.append(0)
  matrix.append(row)

sd_matrix = np.array(matrix)
print("\n Shingle - doc matrix:")
print(sd_matrix)

num_shingles, num_docs = sd_matrix.shape
num_hash = 5

signature_matrix = np.full((num_hash, num_docs), np.inf)

p = 2*num_shingles

hash_functions = [(random.randint(1, p-1), random.randint(0, p-1)) for _ in range(num_hash)]
print("\nHash functions:")
print(hash_functions)

for i , (a,b) in enumerate(hash_functions):
  for row in range(num_shingles):
    hash_val = ((a*row) + b) % p
    for col in range(len(docs)):
      if sd_matrix[row][col]==1:
        if hash_val < signature_matrix[i][col]:
          signature_matrix[i][col] = hash_val

print("\nSignature matrix")
print(signature_matrix)


#Jaccard similarity between the documents

print(f"Jaccard similarity : doc0 vs doc1: {jaccard_similarity(signature_matrix, 0, 1)} ")
print(f"Jaccard similarity : doc0 vs doc2: {jaccard_similarity(signature_matrix, 0, 2)} ")
print(f"Jaccard similarity : doc1 vs doc2: {jaccard_similarity(signature_matrix, 1, 2)} ")

# 3. Cosine similarity and euclidean distance

def cosine_similarity(a,b):
  return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))


def euclidean_distance(a,b):
  return np.linalg.norm(a-b)


docs = [
    "this is a cat",
    "this is a dog",
    "cats and dogs are animals",
    "the dog chased the cat"
]

doc_join = " ".join(docs)
terms = sorted(set(doc_join.split()))
print(terms)

#tf matrix (rows -> docs , cols -> terms)

tf = []
for doc in docs:
  doc_list = doc.split()
  row = []
  for term in terms:
    row.append(doc_list.count(term))
  tf.append(row)

tf_matrix = np.array(tf)

print("TF matrix")
print(tf_matrix)

#idf

N = len(docs)
idf = np.log(N / np.count_nonzero(tf, axis = 0))
print("\nIDF ")
print(idf)

#tf - idf

tf_idf = tf_matrix * idf
print("\nTF-IDF matrix")
print(tf_idf)

#Similarities:

for i in range(len(docs)):
  for j in range(i+1, len(docs)):
    cos_sim = cosine_similarity(tf_idf[i], tf_idf[j])
    euc_dist = euclidean_distance(tf_idf[i], tf_idf[j])
    print(f"\n\nDoc{i} vs Doc{j}:")
    print(f"Cosine similarity: {cos_sim}")
    print(f"Euclidan distance: {euc_dist}")

"""##Pagerank

"""

# Pagerank using power iteration method
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx


def pagerank(adj_matrix, alpha=0.85, tol=1e-6, max_iters = 100):
  n = len(adj_matrix[0])

  #HYPERLINK MATRIX
  H = np.zeros((n,n))
  for i in range(n):
    outlinks = np.sum(adj_matrix[i])
    if outlinks>0:
      for j in range(n):
        H[i][j] = adj_matrix[i][j]/outlinks

  #dangling nodes
  for i in range(n):
    if np.sum(H[i])==0:
      for j in range(n):
        H[i][j] = 1/n

  print("\nHyperlink matrix")
  print(H)

  #G matrix
  G = (alpha * H) + (1-alpha) * (np.ones((n,n))/n)

  print("\nG matrix")
  print(G)

  #power iteration method

  rank = np.ones(n)/n
  for i in range(max_iters):
    new_rank = rank @ G
    if np.linalg.norm(new_rank-rank, 1) < tol:
      break

    rank = new_rank

  return rank



#main function

G = nx.DiGraph()

edges = [(1, 2), (2, 5), (3, 1), (3, 2), (3, 4), (3, 5),
    (4, 5), (5, 4)]

G.add_edges_from(edges)

#visualize graph
plt.figure(figsize=(5,5))
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels = True, node_size=1000, node_color="lightgreen", arrows=True)


#adjacency matrix
adj_matrix = nx.to_numpy_array(G, nodelist=sorted(G.nodes()), dtype=int)
print("\n\nAdjacency matrix:")
print(adj_matrix)


#pagerank
pagerank_scores = pagerank(adj_matrix)
print("\n Page rank scores")
print(pagerank_scores)

print(f"\nSum of pagerank scores: {sum(pagerank_scores)}")

# Pagerank using eigen vector method

import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

def pagerank(adj_matrix, alpha=0.85):
  n = len(adj_matrix[0])

  #HYPERLINK MATRIX
  H = np.zeros((n,n))
  for i in range(n):
    outlinks = np.sum(adj_matrix[i])
    if outlinks>0:
      for j in range(n):
        H[i][j] = adj_matrix[i][j]/outlinks

  #dangling nodes
  for i in range(n):
    if np.sum(H[i])==0:
      for j in range(n):
        H[i][j] = 1/n

  print("\nHyperlink matrix")
  print(H)

  #G matrix
  G = (alpha * H) + (1-alpha) * (np.ones((n,n))/n)

  print("\nG matrix")
  print(G)

  eig_vals, eig_vecs = np.linalg.eig(G.T)

  idx = np.argmin(np.abs(eig_vals-1))

  principal_eig_vector = np.real(eig_vecs[:,idx])

  pagerank = principal_eig_vector/np.sum(principal_eig_vector)
  return pagerank



#main function

G = nx.DiGraph()

edges = [(1, 2), (2, 5), (3, 1), (3, 2), (3, 4), (3, 5),
    (4, 5), (5, 4)]

G.add_edges_from(edges)

#visualize graph
plt.figure(figsize=(5,5))
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels = True, node_size=1000, node_color="lightgreen", arrows=True)


#adjacency matrix
adj_matrix = nx.to_numpy_array(G, nodelist=sorted(G.nodes()), dtype=int)
print("\n\nAdjacency matrix:")
print(adj_matrix)


#pagerank
pagerank_scores = pagerank(adj_matrix)
print("\n Page rank scores")
print(pagerank_scores)

print(f"\nSum of pagerank scores: {sum(pagerank_scores)}")

"""##LSI - SVD"""

import numpy as np

def cosine_similarity(a,b):
  return np.dot(a,b)/ (np.linalg.norm(a)*np.linalg.norm(b))


# docs = [
#     "the cat sat on the mat",
#     "the dog sat on the log",
#     "the cat played with the dog",
#     "dogs and cats are friends"
# ]


docs = [
    "I like cats" , "I like dogs", "cats and dogs"
]


doc_join_list = " ".join(docs).lower().split()
terms = sorted(set(doc_join_list))
print("Unique terms")
print(terms)

term_index={}
for i, term in enumerate(terms):
  term_index[term]=i

print("\nTerm index")
print(term_index)

#Term document matrix
#rows -> terms, cols -> docs

A = np.zeros((len(terms), len(docs)), dtype=float)

for j, doc in enumerate(docs):
  doc_list = doc.lower().split()
  for word in doc_list:
    A[term_index[word]][j] += 1

print("\nTerm document matrix:")
print(A)

#SVD

U, s, V_T = np.linalg.svd(A, full_matrices=False)

sigma = np.diag(s)

print("\nU (terms -> concepts):\n", U)
print("\nΣ (singular values):\n", sigma)
print("\nV^T (docs -> concepts):\n", V_T)

#dimensionality reduction

k=2
U_k = U[:, :k]
sigma_k = sigma[:k , :k]
Vk_T = V_T[:k, :]
print("\nAfter dimensionality reduction")
print("\nU_k (terms -> concepts):\n", U_k)
print("\nΣ_k (singular values):\n", sigma_k)
print("\nV_k^T (docs -> concepts):\n", Vk_T)

doc_vectors = np.dot(sigma_k, Vk_T).T
print("\nReduced Document Representations (LSI space):\n", doc_vectors)


#Query projection
query = "cat and dog play together"
q_vec = np.zeros((len(terms), 1))

q_split = query.lower().split()

for term in q_split:
  if term in term_index:
    q_vec[term_index[term]][0] += 1

print("\nQuery vector")
print(q_vec)

q_dash = np.dot(np.dot(q_vec.T, U_k), np.linalg.inv(sigma_k))
print("\n Query vector projected")
print(q_dash)

#Cosine Similarity between the query and the doc vectors

for i, doc_vec in enumerate(doc_vectors):
  cos_sim = cosine_similarity(q_dash.flatten(), doc_vec)
  print(f"\n\nCosine similarity : query vs doc{i} -> {cos_sim}")